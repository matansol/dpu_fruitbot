{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f4e51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matan\\anaconda3\\envs\\procgen_env_clone\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import dpu_clf\n",
    "import gym\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59168fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prebuilt binaries from: c:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\.build\\relwithdebinfo\\RelWithDebInfo\n"
     ]
    }
   ],
   "source": [
    "seed = 8798\n",
    "\n",
    "env_kwargs = {\n",
    "    \"render_mode\": \"rgb_array\",\n",
    "    \"num_levels\": 0,\n",
    "    \"start_level\": 0,\n",
    "    \"distribution_mode\": \"easy\",\n",
    "    \"rand_seed\": seed,\n",
    "    \"use_discrete_action_wrapper\": True, \n",
    "    \"use_stay_bonus_wrapper\": False,\n",
    "    'food_diversity': 4,\n",
    "    'fruitbot_num_walls': 5,\n",
    "    'fruitbot_num_good_min': 5,\n",
    "    'fruitbot_num_good_range': 1,\n",
    "    'fruitbot_num_bad_min': 5,\n",
    "    'fruitbot_num_bad_range': 1,\n",
    "    'fruitbot_wall_gap_pct': 50,\n",
    "    'fruitbot_door_prob_pct': 0,\n",
    "    \"fruitbot_reward_positive\": 2,\n",
    "    \"fruitbot_reward_negative\": -1,\n",
    "    \"fruitbot_reward_wall_hit\": -2,\n",
    "    }\n",
    "\n",
    "\n",
    "env = gym.make(\"procgen:procgen-fruitbot-v0\", **env_kwargs)\n",
    "\n",
    "# model_path = \"models\\\\fruitbot\\\\20251214-084655_easy\\\\ppo_final.zip\"\n",
    "# model_path2 = \"models\\\\fruitbot\\\\20251213-212435_easy\\\\ppo_final.zip\"\n",
    "# agent = dpu_clf.load_agent(None, model_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64693ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from stable_baselines3 import PPO\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d4f1d",
   "metadata": {},
   "source": [
    "## Detailed Episode Data\n",
    "\n",
    "You can access detailed per-episode data for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f85a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 model directories:\n",
      "  ✓ 20251130-001800_easy\n",
      "  ✓ 20251201-110009_easy\n",
      "  ✓ 20251215-094437_easy\n",
      "  ✓ 20251222-194508_easy\n",
      "  ✓ 20251223-133810_easy\n",
      "  ✓ 20251224-140335_D2_easy\n",
      "  ✓ 20251225-155401_hard\n",
      "  ✓ 20251226-014845_hard\n",
      "  ✓ 20251227-205223_hard\n",
      "  ✓ 20251230-134845_easy\n",
      "  ✓ 20251231-174002_easy\n",
      "  ✗ 20260101-112255_easy\n",
      "  ✓ 20260101-112929_easy\n",
      "  ✓ 20260101-150436_easy\n",
      "  ✓ 20260101-165959_easy\n",
      "  ✗ 20260103-073446_easy\n"
     ]
    }
   ],
   "source": [
    "# Find all models in the fruitbot folder\n",
    "models_base_dir = Path(\"models/fruitbot\")\n",
    "model_dirs = sorted([d for d in models_base_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "print(f\"Found {len(model_dirs)} model directories:\")\n",
    "for model_dir in model_dirs:\n",
    "    model_file = model_dir / \"ppo_final.zip\"\n",
    "    exists = \"✓\" if model_file.exists() else \"✗\"\n",
    "    print(f\"  {exists} {model_dir.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25aea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, env, num_episodes=10, \n",
    "                   fruitbot_reward_positive=2.0,\n",
    "                   fruitbot_reward_negative=-1.0,\n",
    "                   fruitbot_reward_wall_hit=-2.0):\n",
    "    \"\"\"\n",
    "    Evaluate a single model and return episode statistics.;\n",
    "    \n",
    "    Returns:\n",
    "        dict with per-episode data and summary statistics\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = PPO.load(model_path)\n",
    "    \n",
    "    # Track per-episode data\n",
    "    episodes_data = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        if isinstance(obs, tuple):\n",
    "            obs, _ = obs\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        good_food = 0\n",
    "        bad_food = 0\n",
    "        wall_hits = 0\n",
    "        first_frame = True\n",
    "        while not (done or truncated):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            result = env.step(action)\n",
    "            \n",
    "            # Handle both gym and gymnasium API\n",
    "            if len(result) == 4:\n",
    "                obs, reward, done, info = result\n",
    "                truncated = False\n",
    "            else:\n",
    "                obs, reward, done, truncated, info = result\n",
    "\n",
    "            # if first_frame:\n",
    "            #     game_image = info.get('rgb', None)\n",
    "            #     game_image = Image.fromarray(game_image).convert('RGB').resize((512, 512))\n",
    "            #     display(game_image)\n",
    "            #     first_frame = False\n",
    "\n",
    "            \n",
    "            # Ensure reward is a scalar float\n",
    "            try:\n",
    "                r = float(np.asarray(reward).item())\n",
    "            except Exception:\n",
    "                r = float(reward)\n",
    "            \n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "            # Count events based on reward values\n",
    "            TOL = 1e-3\n",
    "            if np.isclose(r, fruitbot_reward_positive, atol=TOL, rtol=0.0):\n",
    "                good_food += 1\n",
    "            elif np.isclose(r, fruitbot_reward_negative, atol=TOL, rtol=0.0):\n",
    "                bad_food += 1\n",
    "            elif np.isclose(r, fruitbot_reward_wall_hit, atol=TOL, rtol=0.0):\n",
    "                wall_hits += 1\n",
    "        \n",
    "        episodes_data.append({\n",
    "            'episode': ep + 1,\n",
    "            'reward': total_reward,\n",
    "            'steps': steps,\n",
    "            'good_food': good_food,\n",
    "            'bad_food': bad_food,\n",
    "            'wall_hits': wall_hits\n",
    "        })\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    rewards = [ep['reward'] for ep in episodes_data]\n",
    "    lengths = [ep['steps'] for ep in episodes_data]\n",
    "    good_foods = [ep['good_food'] for ep in episodes_data]\n",
    "    bad_foods = [ep['bad_food'] for ep in episodes_data]\n",
    "    wall_hits_list = [ep['wall_hits'] for ep in episodes_data]\n",
    "    \n",
    "    summary = {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'mean_steps': np.mean(lengths),\n",
    "        'mean_good_food': np.mean(good_foods),\n",
    "        'mean_bad_food': np.mean(bad_foods),\n",
    "        'mean_wall_hits': np.mean(wall_hits_list),\n",
    "        'episodes_data': episodes_data\n",
    "    }\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f519d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prebuilt binaries from: c:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\.build\\relwithdebinfo\\RelWithDebInfo\n",
      "Using prebuilt binaries from: c:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\.build\\relwithdebinfo\\RelWithDebInfo\n",
      "\n",
      "[1/16] Evaluating 20251130-001800_easy...\n",
      "  Mean reward: 16.27 ± 8.66\n",
      "  Mean steps: 130.7\n",
      "  Mean good food: 1.87\n",
      "  Mean bad food: 1.53\n",
      "  Mean wall hits: 0.07\n",
      "\n",
      "[2/16] Evaluating 20251201-110009_easy...\n",
      "  Mean reward: 9.60 ± 10.50\n",
      "  Mean steps: 120.7\n",
      "  Mean good food: 2.03\n",
      "  Mean bad food: 1.90\n",
      "  Mean wall hits: 0.27\n",
      "\n",
      "[3/16] Evaluating 20251215-094437_easy...\n",
      "  Mean reward: 17.07 ± 11.15\n",
      "  Mean steps: 126.8\n",
      "  Mean good food: 2.70\n",
      "  Mean bad food: 1.17\n",
      "  Mean wall hits: 0.13\n",
      "\n",
      "[4/16] Evaluating 20251222-194508_easy...\n",
      "  Mean reward: 15.00 ± 10.53\n",
      "  Mean steps: 114.7\n",
      "  Mean good food: 2.27\n",
      "  Mean bad food: 1.53\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[5/16] Evaluating 20251223-133810_easy...\n",
      "  Mean reward: 9.43 ± 11.70\n",
      "  Mean steps: 107.2\n",
      "  Mean good food: 1.70\n",
      "  Mean bad food: 1.77\n",
      "  Mean wall hits: 0.33\n",
      "\n",
      "[6/16] Evaluating 20251224-140335_D2_easy...\n",
      "  Mean reward: 17.43 ± 10.61\n",
      "  Mean steps: 122.6\n",
      "  Mean good food: 3.43\n",
      "  Mean bad food: 1.90\n",
      "  Mean wall hits: 0.17\n",
      "\n",
      "[7/16] Evaluating 20251225-155401_hard...\n",
      "  Mean reward: 21.77 ± 8.78\n",
      "  Mean steps: 125.8\n",
      "  Mean good food: 3.87\n",
      "  Mean bad food: 0.40\n",
      "  Mean wall hits: 0.10\n",
      "\n",
      "[8/16] Evaluating 20251226-014845_hard...\n",
      "  Mean reward: 13.03 ± 10.63\n",
      "  Mean steps: 112.1\n",
      "  Mean good food: 3.17\n",
      "  Mean bad food: 3.57\n",
      "  Mean wall hits: 0.23\n",
      "\n",
      "[9/16] Evaluating 20251227-205223_hard...\n",
      "  Mean reward: 21.97 ± 9.49\n",
      "  Mean steps: 123.9\n",
      "  Mean good food: 3.50\n",
      "  Mean bad food: 0.27\n",
      "  Mean wall hits: 0.10\n",
      "\n",
      "[10/16] Evaluating 20251230-134845_easy...\n",
      "  Mean reward: 17.70 ± 10.52\n",
      "  Mean steps: 121.4\n",
      "  Mean good food: 3.23\n",
      "  Mean bad food: 1.30\n",
      "  Mean wall hits: 0.17\n",
      "\n",
      "[11/16] Evaluating 20251231-174002_easy...\n",
      "  Mean reward: 18.13 ± 12.20\n",
      "  Mean steps: 113.5\n",
      "  Mean good food: 3.87\n",
      "  Mean bad food: 0.50\n",
      "  Mean wall hits: 0.23\n",
      "Skipping 20260101-112255_easy - model file not found\n",
      "\n",
      "[13/16] Evaluating 20260101-112929_easy...\n",
      "  Mean reward: 12.57 ± 10.57\n",
      "  Mean steps: 113.1\n",
      "  Mean good food: 0.40\n",
      "  Mean bad food: 0.53\n",
      "  Mean wall hits: 0.23\n",
      "\n",
      "[14/16] Evaluating 20260101-150436_easy...\n",
      "  Mean reward: 16.17 ± 7.96\n",
      "  Mean steps: 128.9\n",
      "  Mean good food: 3.70\n",
      "  Mean bad food: 3.93\n",
      "  Mean wall hits: 0.10\n",
      "\n",
      "[15/16] Evaluating 20260101-165959_easy...\n",
      "  Mean reward: 11.23 ± 10.28\n",
      "  Mean steps: 116.8\n",
      "  Mean good food: 3.50\n",
      "  Mean bad food: 4.00\n",
      "  Mean wall hits: 0.20\n",
      "Skipping 20260103-073446_easy - model file not found\n",
      "\n",
      "============================================================\n",
      "Evaluation complete! Evaluated 14 models.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#create the environment\n",
    "seed = 8798\n",
    "# Use the reward values from your env_kwargs or set them explicitly\n",
    "fruitbot_reward_positive = 2.0\n",
    "fruitbot_reward_negative = -2.0\n",
    "fruitbot_reward_wall_hit = -5.0\n",
    "\n",
    "env_kwargs = {\n",
    "    \"render_mode\": \"rgb_array\",\n",
    "    \"num_levels\": 0,\n",
    "    \"start_level\": 0,\n",
    "    \"distribution_mode\": \"easy\",\n",
    "    \"rand_seed\": 0,\n",
    "    \"use_discrete_action_wrapper\": True, \n",
    "    \"use_stay_bonus_wrapper\": False,\n",
    "    'food_diversity': 4,\n",
    "    'fruitbot_num_walls': 5,\n",
    "    'fruitbot_num_good_min': 5,\n",
    "    'fruitbot_num_good_range': 1,\n",
    "    'fruitbot_num_bad_min': 5,\n",
    "    'fruitbot_num_bad_range': 1,\n",
    "    'fruitbot_wall_gap_pct': 50,\n",
    "    'fruitbot_door_prob_pct': 10,\n",
    "    \"fruitbot_reward_positive\": fruitbot_reward_positive,\n",
    "    \"fruitbot_reward_negative\": fruitbot_reward_negative,\n",
    "    \"fruitbot_reward_wall_hit\": fruitbot_reward_wall_hit,\n",
    "    }\n",
    "\n",
    "hard_env_kwargs = env_kwargs.copy()\n",
    "hard_env_kwargs['distribution_mode'] = 'hard'\n",
    "\n",
    "\n",
    "env_easy = gym.make(\"procgen-fruitbot-v0\", **env_kwargs)\n",
    "env_hard = gym.make(\"procgen-fruitbot-v0\", **hard_env_kwargs)\n",
    "\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "for i, model_dir in enumerate(model_dirs):\n",
    "    model_file = model_dir / \"ppo_final.zip\"\n",
    "    \n",
    "    if not model_file.exists():\n",
    "        print(f\"Skipping {model_dir.name} - model file not found\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n[{i+1}/{len(model_dirs)}] Evaluating {model_dir.name}...\")\n",
    "        env = env_easy if 'easy' in model_dir.name else env_hard\n",
    "        # Evaluate model\n",
    "        eval_results = evaluate_model(\n",
    "            model_path=str(model_file).replace('.zip', ''),  # PPO.load adds .zip automatically\n",
    "            env=env,\n",
    "            num_episodes=30,\n",
    "            fruitbot_reward_positive=fruitbot_reward_positive,\n",
    "            fruitbot_reward_negative=fruitbot_reward_negative,\n",
    "            fruitbot_reward_wall_hit=fruitbot_reward_wall_hit\n",
    "        )\n",
    "        \n",
    "        # Store summary results\n",
    "        results.append({\n",
    "            'model_name': model_dir.name,\n",
    "            'mean_reward': eval_results['mean_reward'],\n",
    "            'std_reward': eval_results['std_reward'],\n",
    "            'mean_steps': eval_results['mean_steps'],\n",
    "            'mean_good_food': eval_results['mean_good_food'],\n",
    "            'mean_bad_food': eval_results['mean_bad_food'],\n",
    "            'mean_wall_hits': eval_results['mean_wall_hits'],\n",
    "            'episodes_data': eval_results['episodes_data']\n",
    "        })\n",
    "        \n",
    "        print(f\"  Mean reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}\")\n",
    "        print(f\"  Mean steps: {eval_results['mean_steps']:.1f}\")\n",
    "        print(f\"  Mean good food: {eval_results['mean_good_food']:.2f}\")\n",
    "        print(f\"  Mean bad food: {eval_results['mean_bad_food']:.2f}\")\n",
    "        print(f\"  Mean wall hits: {eval_results['mean_wall_hits']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_dir.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Evaluation complete! Evaluated {len(results)} models.\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7feeef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of all models (sorted by mean reward):\n",
      "             model_name  mean_reward  std_reward  mean_steps  mean_good_food  mean_bad_food  mean_wall_hits\n",
      "   20251227-205223_hard    21.966667    9.492043  123.900000        3.500000       0.266667        0.100000\n",
      "   20251225-155401_hard    21.766667    8.781357  125.800000        3.866667       0.400000        0.100000\n",
      "   20251231-174002_easy    18.133333   12.203096  113.466667        3.866667       0.500000        0.233333\n",
      "   20251230-134845_easy    17.700000   10.517129  121.366667        3.233333       1.300000        0.166667\n",
      "20251224-140335_D2_easy    17.433333   10.607178  122.633333        3.433333       1.900000        0.166667\n",
      "   20251215-094437_easy    17.066667   11.153276  126.800000        2.700000       1.166667        0.133333\n",
      "   20251130-001800_easy    16.266667    8.663846  130.733333        1.866667       1.533333        0.066667\n",
      "   20260101-150436_easy    16.166667    7.962761  128.933333        3.700000       3.933333        0.100000\n",
      "   20251222-194508_easy    15.000000   10.526158  114.666667        2.266667       1.533333        0.200000\n",
      "   20251226-014845_hard    13.033333   10.625389  112.133333        3.166667       3.566667        0.233333\n",
      "   20260101-112929_easy    12.566667   10.566246  113.133333        0.400000       0.533333        0.233333\n",
      "   20260101-165959_easy    11.233333   10.278402  116.800000        3.500000       4.000000        0.200000\n",
      "   20251201-110009_easy     9.600000   10.499524  120.733333        2.033333       1.900000        0.266667\n",
      "   20251223-133810_easy     9.433333   11.698101  107.166667        1.700000       1.766667        0.333333\n",
      "\n",
      "Results saved to 'model_evaluation_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'model_name': r['model_name'],\n",
    "        'mean_reward': r['mean_reward'],\n",
    "        'std_reward': r['std_reward'],\n",
    "        'mean_steps': r['mean_steps'],\n",
    "        'mean_good_food': r['mean_good_food'],\n",
    "        'mean_bad_food': r['mean_bad_food'],\n",
    "        'mean_wall_hits': r['mean_wall_hits']\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "# Sort by mean reward (descending)\n",
    "summary_df = summary_df.sort_values('mean_reward', ascending=False)\n",
    "\n",
    "print(\"Summary of all models (sorted by mean reward):\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv('model_evaluation_summary.csv', index=False)\n",
    "print(\"\\nResults saved to 'model_evaluation_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91290f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed episodes for best model: 20251227-205223_hard\n",
      "\n",
      "   episode  reward  steps  good_food  bad_food  wall_hits\n",
      "0        1     1.0     80          3         0          1\n",
      "1        2    28.0    132          4         0          0\n",
      "2        3     8.0    132          1         2          0\n",
      "3        4    28.0    132          4         0          0\n",
      "4        5    28.0    132          4         0          0\n",
      "5        6    28.0    132          4         0          0\n",
      "6        7    26.0    132          3         0          0\n",
      "7        8    28.0    132          4         0          0\n",
      "8        9    30.0    132          5         0          0\n",
      "9       10    -5.0     22          0         0          1\n",
      "\n",
      "Detailed episode data saved to 'model_evaluation_detailed.csv'\n",
      "Total episodes: 420\n"
     ]
    }
   ],
   "source": [
    "# Example: View detailed episodes for the best model\n",
    "if results:\n",
    "    best_model = summary_df.iloc[0]['model_name']\n",
    "    best_model_data = next(r for r in results if r['model_name'] == best_model)\n",
    "    \n",
    "    print(f\"Detailed episodes for best model: {best_model}\\n\")\n",
    "    episodes_df = pd.DataFrame(best_model_data['episodes_data'])\n",
    "    print(episodes_df.head(10))\n",
    "    \n",
    "    # Save detailed data for all models\n",
    "    all_episodes = []\n",
    "    for r in results:\n",
    "        for ep_data in r['episodes_data']:\n",
    "            all_episodes.append({\n",
    "                'model_name': r['model_name'],\n",
    "                **ep_data\n",
    "            })\n",
    "    \n",
    "    all_episodes_df = pd.DataFrame(all_episodes)\n",
    "    all_episodes_df.to_csv('model_evaluation_detailed.csv', index=False)\n",
    "    print(f\"\\nDetailed episode data saved to 'model_evaluation_detailed.csv'\")\n",
    "    print(f\"Total episodes: {len(all_episodes_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen_env_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
