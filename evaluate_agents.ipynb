{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f4e51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matan\\anaconda3\\envs\\procgen_env_clone\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import dpu_clf\n",
    "import gym\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59168fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prebuilt binaries from: c:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\.build\\relwithdebinfo\\RelWithDebInfo\n"
     ]
    }
   ],
   "source": [
    "seed = 8798\n",
    "\n",
    "env_kwargs = {\n",
    "    \"render_mode\": \"rgb_array\",\n",
    "    \"num_levels\": 0,\n",
    "    \"start_level\": 0,\n",
    "    \"distribution_mode\": \"easy\",\n",
    "    \"rand_seed\": seed,\n",
    "    \"use_discrete_action_wrapper\": True, \n",
    "    \"use_stay_bonus_wrapper\": False,\n",
    "    'food_diversity': 4,\n",
    "    'fruitbot_num_walls': 5,\n",
    "    'fruitbot_num_good_min': 5,\n",
    "    'fruitbot_num_good_range': 1,\n",
    "    'fruitbot_num_bad_min': 5,\n",
    "    'fruitbot_num_bad_range': 1,\n",
    "    'fruitbot_wall_gap_pct': 50,\n",
    "    'fruitbot_door_prob_pct': 0,\n",
    "    \"fruitbot_reward_positive\": 2,\n",
    "    \"fruitbot_reward_negative\": -1,\n",
    "    \"fruitbot_reward_wall_hit\": -2,\n",
    "    }\n",
    "\n",
    "\n",
    "env = gym.make(\"procgen:procgen-fruitbot-v0\", **env_kwargs)\n",
    "\n",
    "# model_path = \"models\\\\fruitbot\\\\20251214-084655_easy\\\\ppo_final.zip\"\n",
    "# model_path2 = \"models\\\\fruitbot\\\\20251213-212435_easy\\\\ppo_final.zip\"\n",
    "# agent = dpu_clf.load_agent(None, model_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64693ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from stable_baselines3 import PPO\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d4f1d",
   "metadata": {},
   "source": [
    "## Detailed Episode Data\n",
    "\n",
    "You can access detailed per-episode data for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f85a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 model directories:\n",
      "  ✓ 20251126-215809_easy\n",
      "  ✓ 20251127-165318_easy\n",
      "  ✓ 20251130-001800_easy\n",
      "  ✓ 20251130-093847_easy\n",
      "  ✓ 20251201-002723_easy\n",
      "  ✓ 20251201-110009_easy\n",
      "  ✓ 20251203-132922_easy\n",
      "  ✓ 20251213-212435_easy\n",
      "  ✓ 20251214-084655_easy\n",
      "  ✓ 20251215-094437_easy\n",
      "  ✓ 20251215-173855_easy\n",
      "  ✓ 20251216-164146_easy\n",
      "  ✓ 20251216-200621_easy\n",
      "  ✓ 20251222-161336_easy\n",
      "  ✓ 20251222-194508_easy\n",
      "  ✓ 20251223-084234_easy\n",
      "  ✓ 20251223-133810_easy\n",
      "  ✓ 20251224-103651_easy\n",
      "  ✓ 20251224-133036_easy\n",
      "  ✓ 20251224-140335_easy\n",
      "  ✓ 20251225-000925_easy\n",
      "  ✓ 20251225-083104_easy\n",
      "  ✓ 20251225-155401_hard\n",
      "  ✓ 20251226-014845_hard\n"
     ]
    }
   ],
   "source": [
    "# Find all models in the fruitbot folder\n",
    "models_base_dir = Path(\"models/fruitbot\")\n",
    "model_dirs = sorted([d for d in models_base_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "print(f\"Found {len(model_dirs)} model directories:\")\n",
    "for model_dir in model_dirs:\n",
    "    model_file = model_dir / \"ppo_final.zip\"\n",
    "    exists = \"✓\" if model_file.exists() else \"✗\"\n",
    "    print(f\"  {exists} {model_dir.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25aea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, env, num_episodes=10, \n",
    "                   fruitbot_reward_positive=2.0,\n",
    "                   fruitbot_reward_negative=-1.0,\n",
    "                   fruitbot_reward_wall_hit=-2.0):\n",
    "    \"\"\"\n",
    "    Evaluate a single model and return episode statistics.;\n",
    "    \n",
    "    Returns:\n",
    "        dict with per-episode data and summary statistics\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = PPO.load(model_path)\n",
    "    \n",
    "    # Track per-episode data\n",
    "    episodes_data = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        if isinstance(obs, tuple):\n",
    "            obs, _ = obs\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        good_food = 0\n",
    "        bad_food = 0\n",
    "        wall_hits = 0\n",
    "        first_frame = True\n",
    "        while not (done or truncated):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            result = env.step(action)\n",
    "            \n",
    "            # Handle both gym and gymnasium API\n",
    "            if len(result) == 4:\n",
    "                obs, reward, done, info = result\n",
    "                truncated = False\n",
    "            else:\n",
    "                obs, reward, done, truncated, info = result\n",
    "\n",
    "            # if first_frame:\n",
    "            #     game_image = info.get('rgb', None)\n",
    "            #     game_image = Image.fromarray(game_image).convert('RGB').resize((512, 512))\n",
    "            #     display(game_image)\n",
    "            #     first_frame = False\n",
    "\n",
    "            \n",
    "            # Ensure reward is a scalar float\n",
    "            try:\n",
    "                r = float(np.asarray(reward).item())\n",
    "            except Exception:\n",
    "                r = float(reward)\n",
    "            \n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "            # Count events based on reward values\n",
    "            TOL = 1e-3\n",
    "            if np.isclose(r, fruitbot_reward_positive, atol=TOL, rtol=0.0):\n",
    "                good_food += 1\n",
    "            elif np.isclose(r, fruitbot_reward_negative, atol=TOL, rtol=0.0):\n",
    "                bad_food += 1\n",
    "            elif np.isclose(r, fruitbot_reward_wall_hit, atol=TOL, rtol=0.0):\n",
    "                wall_hits += 1\n",
    "        \n",
    "        episodes_data.append({\n",
    "            'episode': ep + 1,\n",
    "            'reward': total_reward,\n",
    "            'steps': steps,\n",
    "            'good_food': good_food,\n",
    "            'bad_food': bad_food,\n",
    "            'wall_hits': wall_hits\n",
    "        })\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    rewards = [ep['reward'] for ep in episodes_data]\n",
    "    lengths = [ep['steps'] for ep in episodes_data]\n",
    "    good_foods = [ep['good_food'] for ep in episodes_data]\n",
    "    bad_foods = [ep['bad_food'] for ep in episodes_data]\n",
    "    wall_hits_list = [ep['wall_hits'] for ep in episodes_data]\n",
    "    \n",
    "    summary = {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'mean_steps': np.mean(lengths),\n",
    "        'mean_good_food': np.mean(good_foods),\n",
    "        'mean_bad_food': np.mean(bad_foods),\n",
    "        'mean_wall_hits': np.mean(wall_hits_list),\n",
    "        'episodes_data': episodes_data\n",
    "    }\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f519d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prebuilt binaries from: c:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\.build\\relwithdebinfo\\RelWithDebInfo\n",
      "\n",
      "[1/24] Evaluating 20251126-215809_easy...\n",
      "  Mean reward: 17.10 ± 7.92\n",
      "  Mean steps: 126.9\n",
      "  Mean good food: 1.20\n",
      "  Mean bad food: 1.37\n",
      "  Mean wall hits: 0.13\n",
      "\n",
      "[2/24] Evaluating 20251127-165318_easy...\n",
      "  Mean reward: 16.13 ± 7.81\n",
      "  Mean steps: 125.3\n",
      "  Mean good food: 1.53\n",
      "  Mean bad food: 1.07\n",
      "  Mean wall hits: 0.10\n",
      "\n",
      "[3/24] Evaluating 20251130-001800_easy...\n",
      "  Mean reward: 18.20 ± 6.30\n",
      "  Mean steps: 132.0\n",
      "  Mean good food: 2.00\n",
      "  Mean bad food: 1.13\n",
      "  Mean wall hits: 0.00\n",
      "\n",
      "[4/24] Evaluating 20251130-093847_easy...\n",
      "  Mean reward: 14.33 ± 10.51\n",
      "  Mean steps: 115.2\n",
      "  Mean good food: 2.07\n",
      "  Mean bad food: 1.40\n",
      "  Mean wall hits: 0.40\n",
      "\n",
      "[5/24] Evaluating 20251201-002723_easy...\n",
      "  Mean reward: 18.50 ± 9.15\n",
      "  Mean steps: 120.8\n",
      "  Mean good food: 2.13\n",
      "  Mean bad food: 1.43\n",
      "  Mean wall hits: 0.17\n",
      "\n",
      "[6/24] Evaluating 20251201-110009_easy...\n",
      "  Mean reward: 11.50 ± 10.43\n",
      "  Mean steps: 104.3\n",
      "  Mean good food: 1.93\n",
      "  Mean bad food: 1.57\n",
      "  Mean wall hits: 0.40\n",
      "\n",
      "[7/24] Evaluating 20251203-132922_easy...\n",
      "  Mean reward: 15.37 ± 8.83\n",
      "  Mean steps: 127.9\n",
      "  Mean good food: 1.57\n",
      "  Mean bad food: 1.03\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[8/24] Evaluating 20251213-212435_easy...\n",
      "  Mean reward: 14.17 ± 8.66\n",
      "  Mean steps: 121.7\n",
      "  Mean good food: 1.53\n",
      "  Mean bad food: 1.17\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[9/24] Evaluating 20251214-084655_easy...\n",
      "  Mean reward: 16.17 ± 7.32\n",
      "  Mean steps: 122.6\n",
      "  Mean good food: 1.93\n",
      "  Mean bad food: 1.17\n",
      "  Mean wall hits: 0.10\n",
      "\n",
      "[10/24] Evaluating 20251215-094437_easy...\n",
      "  Mean reward: 17.90 ± 9.27\n",
      "  Mean steps: 126.0\n",
      "  Mean good food: 2.63\n",
      "  Mean bad food: 1.43\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[11/24] Evaluating 20251215-173855_easy...\n",
      "  Mean reward: 18.33 ± 7.60\n",
      "  Mean steps: 124.0\n",
      "  Mean good food: 1.80\n",
      "  Mean bad food: 1.40\n",
      "  Mean wall hits: 0.10\n",
      "\n",
      "[12/24] Evaluating 20251216-164146_easy...\n",
      "  Mean reward: 19.83 ± 6.16\n",
      "  Mean steps: 129.6\n",
      "  Mean good food: 1.43\n",
      "  Mean bad food: 1.10\n",
      "  Mean wall hits: 0.03\n",
      "\n",
      "[13/24] Evaluating 20251216-200621_easy...\n",
      "  Mean reward: 11.63 ± 7.28\n",
      "  Mean steps: 117.2\n",
      "  Mean good food: 2.73\n",
      "  Mean bad food: 2.40\n",
      "  Mean wall hits: 0.17\n",
      "\n",
      "[14/24] Evaluating 20251222-161336_easy...\n",
      "  Mean reward: 8.83 ± 10.41\n",
      "  Mean steps: 101.4\n",
      "  Mean good food: 1.60\n",
      "  Mean bad food: 1.70\n",
      "  Mean wall hits: 0.53\n",
      "\n",
      "[15/24] Evaluating 20251222-194508_easy...\n",
      "  Mean reward: 16.90 ± 9.26\n",
      "  Mean steps: 119.7\n",
      "  Mean good food: 2.17\n",
      "  Mean bad food: 1.77\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[16/24] Evaluating 20251223-084234_easy...\n",
      "  Mean reward: 6.87 ± 9.92\n",
      "  Mean steps: 86.6\n",
      "  Mean good food: 1.57\n",
      "  Mean bad food: 1.33\n",
      "  Mean wall hits: 0.63\n",
      "\n",
      "[17/24] Evaluating 20251223-133810_easy...\n",
      "  Mean reward: 16.10 ± 9.92\n",
      "  Mean steps: 117.3\n",
      "  Mean good food: 2.37\n",
      "  Mean bad food: 1.87\n",
      "  Mean wall hits: 0.23\n",
      "\n",
      "[18/24] Evaluating 20251224-103651_easy...\n",
      "  Mean reward: 13.37 ± 10.24\n",
      "  Mean steps: 115.3\n",
      "  Mean good food: 3.07\n",
      "  Mean bad food: 3.53\n",
      "  Mean wall hits: 0.23\n",
      "\n",
      "[19/24] Evaluating 20251224-133036_easy...\n",
      "  Mean reward: 21.43 ± 6.57\n",
      "  Mean steps: 128.8\n",
      "  Mean good food: 3.70\n",
      "  Mean bad food: 2.53\n",
      "  Mean wall hits: 0.07\n",
      "\n",
      "[20/24] Evaluating 20251224-140335_easy...\n",
      "  Mean reward: 18.37 ± 9.99\n",
      "  Mean steps: 122.2\n",
      "  Mean good food: 3.60\n",
      "  Mean bad food: 1.67\n",
      "  Mean wall hits: 0.27\n",
      "\n",
      "[21/24] Evaluating 20251225-000925_easy...\n",
      "  Mean reward: 22.23 ± 6.17\n",
      "  Mean steps: 131.1\n",
      "  Mean good food: 3.87\n",
      "  Mean bad food: 2.03\n",
      "  Mean wall hits: 0.07\n",
      "\n",
      "[22/24] Evaluating 20251225-083104_easy...\n",
      "  Mean reward: 22.70 ± 5.80\n",
      "  Mean steps: 128.5\n",
      "  Mean good food: 3.60\n",
      "  Mean bad food: 1.87\n",
      "  Mean wall hits: 0.10\n",
      "\n",
      "[23/24] Evaluating 20251225-155401_hard...\n",
      "  Mean reward: -1.80 ± 1.08\n",
      "  Mean steps: 28.1\n",
      "  Mean good food: 0.23\n",
      "  Mean bad food: 0.33\n",
      "  Mean wall hits: 0.97\n",
      "\n",
      "[24/24] Evaluating 20251226-014845_hard...\n",
      "  Mean reward: 1.13 ± 6.46\n",
      "  Mean steps: 48.5\n",
      "  Mean good food: 1.60\n",
      "  Mean bad food: 1.53\n",
      "  Mean wall hits: 0.93\n",
      "\n",
      "============================================================\n",
      "Evaluation complete! Evaluated 24 models.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#create the environment\n",
    "seed = 8798\n",
    "# Use the reward values from your env_kwargs or set them explicitly\n",
    "fruitbot_reward_positive = 2.0\n",
    "fruitbot_reward_negative = -1.0\n",
    "fruitbot_reward_wall_hit = -2.0\n",
    "\n",
    "env_kwargs = {\n",
    "    \"render_mode\": \"rgb_array\",\n",
    "    \"num_levels\": 0,\n",
    "    \"start_level\": 0,\n",
    "    \"distribution_mode\": \"easy\",\n",
    "    \"rand_seed\": seed,\n",
    "    \"use_discrete_action_wrapper\": True, \n",
    "    \"use_stay_bonus_wrapper\": False,\n",
    "    'food_diversity': 4,\n",
    "    'fruitbot_num_walls': 5,\n",
    "    'fruitbot_num_good_min': 5,\n",
    "    'fruitbot_num_good_range': 1,\n",
    "    'fruitbot_num_bad_min': 5,\n",
    "    'fruitbot_num_bad_range': 1,\n",
    "    'fruitbot_wall_gap_pct': 50,\n",
    "    'fruitbot_door_prob_pct': 0,\n",
    "    \"fruitbot_reward_positive\": fruitbot_reward_positive,\n",
    "    \"fruitbot_reward_negative\": fruitbot_reward_negative,\n",
    "    \"fruitbot_reward_wall_hit\": fruitbot_reward_wall_hit,\n",
    "    }\n",
    "\n",
    "\n",
    "env = gym.make(\"procgen:procgen-fruitbot-v0\", **env_kwargs)\n",
    "\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "for i, model_dir in enumerate(model_dirs):\n",
    "    model_file = model_dir / \"ppo_final.zip\"\n",
    "    \n",
    "    if not model_file.exists():\n",
    "        print(f\"Skipping {model_dir.name} - model file not found\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n[{i+1}/{len(model_dirs)}] Evaluating {model_dir.name}...\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        eval_results = evaluate_model(\n",
    "            model_path=str(model_file).replace('.zip', ''),  # PPO.load adds .zip automatically\n",
    "            env=env,\n",
    "            num_episodes=30,\n",
    "            fruitbot_reward_positive=fruitbot_reward_positive,\n",
    "            fruitbot_reward_negative=fruitbot_reward_negative,\n",
    "            fruitbot_reward_wall_hit=fruitbot_reward_wall_hit\n",
    "        )\n",
    "        \n",
    "        # Store summary results\n",
    "        results.append({\n",
    "            'model_name': model_dir.name,\n",
    "            'mean_reward': eval_results['mean_reward'],\n",
    "            'std_reward': eval_results['std_reward'],\n",
    "            'mean_steps': eval_results['mean_steps'],\n",
    "            'mean_good_food': eval_results['mean_good_food'],\n",
    "            'mean_bad_food': eval_results['mean_bad_food'],\n",
    "            'mean_wall_hits': eval_results['mean_wall_hits'],\n",
    "            'episodes_data': eval_results['episodes_data']\n",
    "        })\n",
    "        \n",
    "        print(f\"  Mean reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}\")\n",
    "        print(f\"  Mean steps: {eval_results['mean_steps']:.1f}\")\n",
    "        print(f\"  Mean good food: {eval_results['mean_good_food']:.2f}\")\n",
    "        print(f\"  Mean bad food: {eval_results['mean_bad_food']:.2f}\")\n",
    "        print(f\"  Mean wall hits: {eval_results['mean_wall_hits']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_dir.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Evaluation complete! Evaluated {len(results)} models.\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7feeef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of all models (sorted by mean reward):\n",
      "          model_name  mean_reward  std_reward  mean_steps  mean_good_food  mean_bad_food  mean_wall_hits\n",
      "20251225-083104_easy    22.700000    5.803160  128.500000        3.600000       1.866667        0.100000\n",
      "20251225-000925_easy    22.233333    6.173510  131.066667        3.866667       2.033333        0.066667\n",
      "20251224-133036_easy    21.433333    6.565990  128.833333        3.700000       2.533333        0.066667\n",
      "20251216-164146_easy    19.833333    6.164865  129.566667        1.433333       1.100000        0.033333\n",
      "20251201-002723_easy    18.500000    9.146037  120.766667        2.133333       1.433333        0.166667\n",
      "20251224-140335_easy    18.366667    9.994943  122.233333        3.600000       1.666667        0.266667\n",
      "20251215-173855_easy    18.333333    7.595320  123.966667        1.800000       1.400000        0.100000\n",
      "20251130-001800_easy    18.200000    6.300265  132.000000        2.000000       1.133333        0.000000\n",
      "20251215-094437_easy    17.900000    9.271282  126.000000        2.633333       1.433333        0.200000\n",
      "20251126-215809_easy    17.100000    7.917702  126.900000        1.200000       1.366667        0.133333\n",
      "20251222-194508_easy    16.900000    9.260490  119.666667        2.166667       1.766667        0.200000\n",
      "20251214-084655_easy    16.166667    7.321581  122.566667        1.933333       1.166667        0.100000\n",
      "20251127-165318_easy    16.133333    7.809111  125.300000        1.533333       1.066667        0.100000\n",
      "20251223-133810_easy    16.100000    9.920853  117.266667        2.366667       1.866667        0.233333\n",
      "20251203-132922_easy    15.366667    8.826035  127.866667        1.566667       1.033333        0.200000\n",
      "20251130-093847_easy    14.333333   10.511370  115.200000        2.066667       1.400000        0.400000\n",
      "20251213-212435_easy    14.166667    8.660575  121.733333        1.533333       1.166667        0.200000\n",
      "20251224-103651_easy    13.366667   10.242016  115.333333        3.066667       3.533333        0.233333\n",
      "20251216-200621_easy    11.633333    7.282323  117.200000        2.733333       2.400000        0.166667\n",
      "20251201-110009_easy    11.500000   10.426728  104.333333        1.933333       1.566667        0.400000\n",
      "20251222-161336_easy     8.833333   10.415000  101.400000        1.600000       1.700000        0.533333\n",
      "20251223-084234_easy     6.866667    9.915420   86.566667        1.566667       1.333333        0.633333\n",
      "20251226-014845_hard     1.133333    6.458758   48.466667        1.600000       1.533333        0.933333\n",
      "20251225-155401_hard    -1.800000    1.077033   28.133333        0.233333       0.333333        0.966667\n",
      "\n",
      "Results saved to 'model_evaluation_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'model_name': r['model_name'],\n",
    "        'mean_reward': r['mean_reward'],\n",
    "        'std_reward': r['std_reward'],\n",
    "        'mean_steps': r['mean_steps'],\n",
    "        'mean_good_food': r['mean_good_food'],\n",
    "        'mean_bad_food': r['mean_bad_food'],\n",
    "        'mean_wall_hits': r['mean_wall_hits']\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "# Sort by mean reward (descending)\n",
    "summary_df = summary_df.sort_values('mean_reward', ascending=False)\n",
    "\n",
    "print(\"Summary of all models (sorted by mean reward):\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv('model_evaluation_summary.csv', index=False)\n",
    "print(\"\\nResults saved to 'model_evaluation_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91290f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed episodes for best model: 20251225-083104_easy\n",
      "\n",
      "   episode  reward  steps  good_food  bad_food  wall_hits\n",
      "0        1    16.0    132          4         2          0\n",
      "1        2    28.0    132          5         2          0\n",
      "2        3    25.0    132          4         3          0\n",
      "3        4    26.0    132          4         2          0\n",
      "4        5    27.0    132          4         1          0\n",
      "5        6    26.0    132          4         2          0\n",
      "6        7    19.0    132          4         0          0\n",
      "7        8     5.0    108          4         1          1\n",
      "8        9    25.0    132          3         1          0\n",
      "9       10    16.0    132          3         0          0\n",
      "\n",
      "Detailed episode data saved to 'model_evaluation_detailed.csv'\n",
      "Total episodes: 720\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Example: View detailed episodes for the best model\n",
    "if results:\n",
    "    best_model = summary_df.iloc[0]['model_name']\n",
    "    best_model_data = next(r for r in results if r['model_name'] == best_model)\n",
    "    \n",
    "    print(f\"Detailed episodes for best model: {best_model}\\n\")\n",
    "    episodes_df = pd.DataFrame(best_model_data['episodes_data'])\n",
    "    print(episodes_df.head(10))\n",
    "    \n",
    "    # Save detailed data for all models\n",
    "    all_episodes = []\n",
    "    for r in results:\n",
    "        for ep_data in r['episodes_data']:\n",
    "            all_episodes.append({\n",
    "                'model_name': r['model_name'],\n",
    "                **ep_data\n",
    "            })\n",
    "    \n",
    "    all_episodes_df = pd.DataFrame(all_episodes)\n",
    "    all_episodes_df.to_csv('model_evaluation_detailed.csv', index=False)\n",
    "    print(f\"\\nDetailed episode data saved to 'model_evaluation_detailed.csv'\")\n",
    "    print(f\"Total episodes: {len(all_episodes_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen_env_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
