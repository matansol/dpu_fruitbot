{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f4e51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matan\\anaconda3\\envs\\procgen_env_clone\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import dpu_clf\n",
    "import gym\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59168fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting make_env with env_name: fruitbot\n",
      "Using prebuilt binaries from: c:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\.build\\relwithdebinfo\\RelWithDebInfo\n",
      "ExtractDictObWrapper applied\n"
     ]
    }
   ],
   "source": [
    "seed = 8798\n",
    "\n",
    "env_kwargs = {\n",
    "    \"render_mode\": \"rgb_array\",\n",
    "    \"num_levels\": 0,\n",
    "    \"start_level\": 0,\n",
    "    \"distribution_mode\": \"easy\",\n",
    "    \"rand_seed\": seed,\n",
    "    \"use_discrete_action_wrapper\": True, \n",
    "    \"use_stay_bonus_wrapper\": False,\n",
    "    'food_diversity': 4,\n",
    "    'fruitbot_num_walls': 5,\n",
    "    'fruitbot_num_good_min': 5,\n",
    "    'fruitbot_num_good_range': 1,\n",
    "    'fruitbot_num_bad_min': 5,\n",
    "    'fruitbot_num_bad_range': 1,\n",
    "    'fruitbot_wall_gap_pct': 50,\n",
    "    'fruitbot_door_prob_pct': 0,\n",
    "    \"fruitbot_reward_positive\": 2,\n",
    "    \"fruitbot_reward_negative\": -1,\n",
    "    \"fruitbot_reward_wall_hit\": -2,\n",
    "    }\n",
    "\n",
    "\n",
    "env = gym.make(\"procgen:procgen-fruitbot-v0\", **env_kwargs)\n",
    "\n",
    "# model_path = \"models\\\\fruitbot\\\\20251214-084655_easy\\\\ppo_final.zip\"\n",
    "# model_path2 = \"models\\\\fruitbot\\\\20251213-212435_easy\\\\ppo_final.zip\"\n",
    "# agent = dpu_clf.load_agent(None, model_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64693ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from stable_baselines3 import PPO\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d4f1d",
   "metadata": {},
   "source": [
    "## Detailed Episode Data\n",
    "\n",
    "You can access detailed per-episode data for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38f85a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 model directories:\n",
      "  ✓ 20251118-115646_easy\n",
      "  ✓ 20251118-164559_easy\n",
      "  ✓ 20251118-191507_easy\n",
      "  ✓ 20251119-112830_easy\n",
      "  ✓ 20251126-215809_easy\n",
      "  ✓ 20251127-165318_easy\n",
      "  ✓ 20251130-001800_easy\n",
      "  ✓ 20251130-093847_easy\n",
      "  ✓ 20251201-002723_easy\n",
      "  ✓ 20251201-110009_easy\n",
      "  ✓ 20251201-191008_easy\n",
      "  ✓ 20251203-104254_easy\n",
      "  ✓ 20251203-132922_easy\n",
      "  ✓ 20251213-212435_easy\n",
      "  ✓ 20251214-084655_easy\n",
      "  ✓ 20251214-120912_easy\n",
      "  ✓ 20251215-094437_easy\n",
      "  ✓ 20251215-173855_easy\n",
      "  ✓ 20251216-164146_easy\n",
      "  ✓ 20251216-200621_easy\n",
      "  ✓ 20251217-003433_easy\n"
     ]
    }
   ],
   "source": [
    "# Find all models in the fruitbot folder\n",
    "models_base_dir = Path(\"models/fruitbot\")\n",
    "model_dirs = sorted([d for d in models_base_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "print(f\"Found {len(model_dirs)} model directories:\")\n",
    "for model_dir in model_dirs:\n",
    "    model_file = model_dir / \"ppo_final.zip\"\n",
    "    exists = \"✓\" if model_file.exists() else \"✗\"\n",
    "    print(f\"  {exists} {model_dir.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f25aea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, env, num_episodes=10, \n",
    "                   fruitbot_reward_positive=2.0,\n",
    "                   fruitbot_reward_negative=-1.0,\n",
    "                   fruitbot_reward_wall_hit=-2.0):\n",
    "    \"\"\"\n",
    "    Evaluate a single model and return episode statistics.;\n",
    "    \n",
    "    Returns:\n",
    "        dict with per-episode data and summary statistics\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = PPO.load(model_path)\n",
    "    \n",
    "    # Track per-episode data\n",
    "    episodes_data = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        if isinstance(obs, tuple):\n",
    "            obs, _ = obs\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        good_food = 0\n",
    "        bad_food = 0\n",
    "        wall_hits = 0\n",
    "        first_frame = True\n",
    "        while not (done or truncated):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            result = env.step(action)\n",
    "            \n",
    "            # Handle both gym and gymnasium API\n",
    "            if len(result) == 4:\n",
    "                obs, reward, done, info = result\n",
    "                truncated = False\n",
    "            else:\n",
    "                obs, reward, done, truncated, info = result\n",
    "\n",
    "            # if first_frame:\n",
    "            #     game_image = info.get('rgb', None)\n",
    "            #     game_image = Image.fromarray(game_image).convert('RGB').resize((512, 512))\n",
    "            #     display(game_image)\n",
    "            #     first_frame = False\n",
    "\n",
    "            \n",
    "            # Ensure reward is a scalar float\n",
    "            try:\n",
    "                r = float(np.asarray(reward).item())\n",
    "            except Exception:\n",
    "                r = float(reward)\n",
    "            \n",
    "            total_reward += r\n",
    "            steps += 1\n",
    "            # Count events based on reward values\n",
    "            TOL = 1e-3\n",
    "            if np.isclose(r, fruitbot_reward_positive, atol=TOL, rtol=0.0):\n",
    "                good_food += 1\n",
    "            elif np.isclose(r, fruitbot_reward_negative, atol=TOL, rtol=0.0):\n",
    "                bad_food += 1\n",
    "            elif np.isclose(r, fruitbot_reward_wall_hit, atol=TOL, rtol=0.0):\n",
    "                wall_hits += 1\n",
    "        \n",
    "        episodes_data.append({\n",
    "            'episode': ep + 1,\n",
    "            'reward': total_reward,\n",
    "            'steps': steps,\n",
    "            'good_food': good_food,\n",
    "            'bad_food': bad_food,\n",
    "            'wall_hits': wall_hits\n",
    "        })\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    rewards = [ep['reward'] for ep in episodes_data]\n",
    "    lengths = [ep['steps'] for ep in episodes_data]\n",
    "    good_foods = [ep['good_food'] for ep in episodes_data]\n",
    "    bad_foods = [ep['bad_food'] for ep in episodes_data]\n",
    "    wall_hits_list = [ep['wall_hits'] for ep in episodes_data]\n",
    "    \n",
    "    summary = {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'mean_steps': np.mean(lengths),\n",
    "        'mean_good_food': np.mean(good_foods),\n",
    "        'mean_bad_food': np.mean(bad_foods),\n",
    "        'mean_wall_hits': np.mean(wall_hits_list),\n",
    "        'episodes_data': episodes_data\n",
    "    }\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23f519d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting make_env with env_name: fruitbot\n",
      "Using prebuilt binaries from: c:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\.build\\relwithdebinfo\\RelWithDebInfo\n",
      "ExtractDictObWrapper applied\n",
      "\n",
      "[1/21] Evaluating 20251118-115646_easy...\n",
      "Error evaluating 20251118-115646_easy: No module named 'gymnasium'\n",
      "\n",
      "[2/21] Evaluating 20251118-164559_easy...\n",
      "Error evaluating 20251118-164559_easy: No module named 'gymnasium'\n",
      "\n",
      "[3/21] Evaluating 20251118-191507_easy...\n",
      "Error evaluating 20251118-191507_easy: No module named 'gymnasium'\n",
      "\n",
      "[4/21] Evaluating 20251119-112830_easy...\n",
      "Error evaluating 20251119-112830_easy: No module named 'gymnasium'\n",
      "\n",
      "[5/21] Evaluating 20251126-135043_easy...\n",
      "  Mean reward: 7.70 ± 11.28\n",
      "  Mean steps: 85.1\n",
      "  Mean good food: 1.70\n",
      "  Mean bad food: 1.20\n",
      "  Mean wall hits: 0.60\n",
      "\n",
      "[6/21] Evaluating 20251126-191339_easy...\n",
      "  Mean reward: 7.70 ± 11.28\n",
      "  Mean steps: 85.1\n",
      "  Mean good food: 1.70\n",
      "  Mean bad food: 1.20\n",
      "  Mean wall hits: 0.60\n",
      "\n",
      "[6/21] Evaluating 20251126-191339_easy...\n",
      "  Mean reward: 20.03 ± 4.09\n",
      "  Mean steps: 96.0\n",
      "  Mean good food: 1.30\n",
      "  Mean bad food: 0.93\n",
      "  Mean wall hits: 0.00\n",
      "\n",
      "[7/21] Evaluating 20251126-215809_easy...\n",
      "  Mean reward: 20.03 ± 4.09\n",
      "  Mean steps: 96.0\n",
      "  Mean good food: 1.30\n",
      "  Mean bad food: 0.93\n",
      "  Mean wall hits: 0.00\n",
      "\n",
      "[7/21] Evaluating 20251126-215809_easy...\n",
      "  Mean reward: 14.03 ± 8.92\n",
      "  Mean steps: 87.0\n",
      "  Mean good food: 1.13\n",
      "  Mean bad food: 1.17\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[8/21] Evaluating 20251127-165318_easy...\n",
      "  Mean reward: 14.03 ± 8.92\n",
      "  Mean steps: 87.0\n",
      "  Mean good food: 1.13\n",
      "  Mean bad food: 1.17\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[8/21] Evaluating 20251127-165318_easy...\n",
      "  Mean reward: 15.47 ± 8.45\n",
      "  Mean steps: 92.1\n",
      "  Mean good food: 1.10\n",
      "  Mean bad food: 1.07\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[9/21] Evaluating 20251130-001800_easy...\n",
      "  Mean reward: 15.47 ± 8.45\n",
      "  Mean steps: 92.1\n",
      "  Mean good food: 1.10\n",
      "  Mean bad food: 1.07\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[9/21] Evaluating 20251130-001800_easy...\n",
      "  Mean reward: 12.47 ± 10.42\n",
      "  Mean steps: 88.5\n",
      "  Mean good food: 1.60\n",
      "  Mean bad food: 1.23\n",
      "  Mean wall hits: 0.37\n",
      "\n",
      "[10/21] Evaluating 20251130-093847_easy...\n",
      "  Mean reward: 12.47 ± 10.42\n",
      "  Mean steps: 88.5\n",
      "  Mean good food: 1.60\n",
      "  Mean bad food: 1.23\n",
      "  Mean wall hits: 0.37\n",
      "\n",
      "[10/21] Evaluating 20251130-093847_easy...\n",
      "  Mean reward: 15.07 ± 9.81\n",
      "  Mean steps: 86.8\n",
      "  Mean good food: 1.93\n",
      "  Mean bad food: 1.63\n",
      "  Mean wall hits: 0.30\n",
      "\n",
      "[11/21] Evaluating 20251201-002723_easy...\n",
      "  Mean reward: 15.07 ± 9.81\n",
      "  Mean steps: 86.8\n",
      "  Mean good food: 1.93\n",
      "  Mean bad food: 1.63\n",
      "  Mean wall hits: 0.30\n",
      "\n",
      "[11/21] Evaluating 20251201-002723_easy...\n",
      "  Mean reward: 19.00 ± 7.85\n",
      "  Mean steps: 91.3\n",
      "  Mean good food: 2.30\n",
      "  Mean bad food: 1.67\n",
      "  Mean wall hits: 0.13\n",
      "\n",
      "[12/21] Evaluating 20251201-110009_easy...\n",
      "  Mean reward: 19.00 ± 7.85\n",
      "  Mean steps: 91.3\n",
      "  Mean good food: 2.30\n",
      "  Mean bad food: 1.67\n",
      "  Mean wall hits: 0.13\n",
      "\n",
      "[12/21] Evaluating 20251201-110009_easy...\n",
      "  Mean reward: 12.47 ± 9.48\n",
      "  Mean steps: 88.6\n",
      "  Mean good food: 2.67\n",
      "  Mean bad food: 1.40\n",
      "  Mean wall hits: 0.33\n",
      "\n",
      "[13/21] Evaluating 20251201-191008_easy...\n",
      "  Mean reward: 12.47 ± 9.48\n",
      "  Mean steps: 88.6\n",
      "  Mean good food: 2.67\n",
      "  Mean bad food: 1.40\n",
      "  Mean wall hits: 0.33\n",
      "\n",
      "[13/21] Evaluating 20251201-191008_easy...\n",
      "  Mean reward: 18.77 ± 6.64\n",
      "  Mean steps: 93.0\n",
      "  Mean good food: 1.47\n",
      "  Mean bad food: 1.00\n",
      "  Mean wall hits: 0.10\n",
      "\n",
      "[14/21] Evaluating 20251203-104254_easy...\n",
      "  Mean reward: 18.77 ± 6.64\n",
      "  Mean steps: 93.0\n",
      "  Mean good food: 1.47\n",
      "  Mean bad food: 1.00\n",
      "  Mean wall hits: 0.10\n",
      "\n",
      "[14/21] Evaluating 20251203-104254_easy...\n",
      "  Mean reward: 13.13 ± 10.03\n",
      "  Mean steps: 85.4\n",
      "  Mean good food: 2.17\n",
      "  Mean bad food: 1.23\n",
      "  Mean wall hits: 0.33\n",
      "\n",
      "[15/21] Evaluating 20251203-132922_easy...\n",
      "  Mean reward: 13.13 ± 10.03\n",
      "  Mean steps: 85.4\n",
      "  Mean good food: 2.17\n",
      "  Mean bad food: 1.23\n",
      "  Mean wall hits: 0.33\n",
      "\n",
      "[15/21] Evaluating 20251203-132922_easy...\n",
      "  Mean reward: 15.97 ± 10.13\n",
      "  Mean steps: 88.7\n",
      "  Mean good food: 1.43\n",
      "  Mean bad food: 1.17\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[16/21] Evaluating 20251213-212435_easy...\n",
      "  Mean reward: 15.97 ± 10.13\n",
      "  Mean steps: 88.7\n",
      "  Mean good food: 1.43\n",
      "  Mean bad food: 1.17\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[16/21] Evaluating 20251213-212435_easy...\n",
      "  Mean reward: 6.70 ± 9.28\n",
      "  Mean steps: 86.5\n",
      "  Mean good food: 1.47\n",
      "  Mean bad food: 1.33\n",
      "  Mean wall hits: 0.63\n",
      "\n",
      "[17/21] Evaluating 20251214-084655_easy...\n",
      "  Mean reward: 6.70 ± 9.28\n",
      "  Mean steps: 86.5\n",
      "  Mean good food: 1.47\n",
      "  Mean bad food: 1.33\n",
      "  Mean wall hits: 0.63\n",
      "\n",
      "[17/21] Evaluating 20251214-084655_easy...\n",
      "  Mean reward: 15.67 ± 8.47\n",
      "  Mean steps: 90.9\n",
      "  Mean good food: 1.50\n",
      "  Mean bad food: 1.37\n",
      "  Mean wall hits: 0.17\n",
      "\n",
      "[18/21] Evaluating 20251214-120912_easy...\n",
      "  Mean reward: 15.67 ± 8.47\n",
      "  Mean steps: 90.9\n",
      "  Mean good food: 1.50\n",
      "  Mean bad food: 1.37\n",
      "  Mean wall hits: 0.17\n",
      "\n",
      "[18/21] Evaluating 20251214-120912_easy...\n",
      "  Mean reward: 16.07 ± 9.82\n",
      "  Mean steps: 85.0\n",
      "  Mean good food: 1.43\n",
      "  Mean bad food: 1.10\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[19/21] Evaluating 20251215-094437_easy...\n",
      "  Mean reward: 16.07 ± 9.82\n",
      "  Mean steps: 85.0\n",
      "  Mean good food: 1.43\n",
      "  Mean bad food: 1.10\n",
      "  Mean wall hits: 0.20\n",
      "\n",
      "[19/21] Evaluating 20251215-094437_easy...\n",
      "  Mean reward: 19.97 ± 7.50\n",
      "  Mean steps: 93.3\n",
      "  Mean good food: 2.17\n",
      "  Mean bad food: 1.67\n",
      "  Mean wall hits: 0.07\n",
      "\n",
      "[20/21] Evaluating 20251215-173855_easy...\n",
      "  Mean reward: 19.97 ± 7.50\n",
      "  Mean steps: 93.3\n",
      "  Mean good food: 2.17\n",
      "  Mean bad food: 1.67\n",
      "  Mean wall hits: 0.07\n",
      "\n",
      "[20/21] Evaluating 20251215-173855_easy...\n",
      "  Mean reward: 17.83 ± 7.16\n",
      "  Mean steps: 91.4\n",
      "  Mean good food: 1.93\n",
      "  Mean bad food: 1.23\n",
      "  Mean wall hits: 0.07\n",
      "Skipping 20251216-164146_easy - model file not found\n",
      "\n",
      "============================================================\n",
      "Evaluation complete! Evaluated 16 models.\n",
      "============================================================\n",
      "  Mean reward: 17.83 ± 7.16\n",
      "  Mean steps: 91.4\n",
      "  Mean good food: 1.93\n",
      "  Mean bad food: 1.23\n",
      "  Mean wall hits: 0.07\n",
      "Skipping 20251216-164146_easy - model file not found\n",
      "\n",
      "============================================================\n",
      "Evaluation complete! Evaluated 16 models.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#create the environment\n",
    "seed = 8798\n",
    "# Use the reward values from your env_kwargs or set them explicitly\n",
    "fruitbot_reward_positive = 2.0\n",
    "fruitbot_reward_negative = -1.0\n",
    "fruitbot_reward_wall_hit = -2.0\n",
    "\n",
    "env_kwargs = {\n",
    "    \"render_mode\": \"rgb_array\",\n",
    "    \"num_levels\": 0,\n",
    "    \"start_level\": 0,\n",
    "    \"distribution_mode\": \"easy\",\n",
    "    \"rand_seed\": seed,\n",
    "    \"use_discrete_action_wrapper\": True, \n",
    "    \"use_stay_bonus_wrapper\": False,\n",
    "    'food_diversity': 4,\n",
    "    'fruitbot_num_walls': 5,\n",
    "    'fruitbot_num_good_min': 5,\n",
    "    'fruitbot_num_good_range': 1,\n",
    "    'fruitbot_num_bad_min': 5,\n",
    "    'fruitbot_num_bad_range': 1,\n",
    "    'fruitbot_wall_gap_pct': 50,\n",
    "    'fruitbot_door_prob_pct': 0,\n",
    "    \"fruitbot_reward_positive\": fruitbot_reward_positive,\n",
    "    \"fruitbot_reward_negative\": fruitbot_reward_negative,\n",
    "    \"fruitbot_reward_wall_hit\": fruitbot_reward_wall_hit,\n",
    "    }\n",
    "\n",
    "\n",
    "env = gym.make(\"procgen:procgen-fruitbot-v0\", **env_kwargs)\n",
    "\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "for i, model_dir in enumerate(model_dirs):\n",
    "    model_file = model_dir / \"ppo_final.zip\"\n",
    "    \n",
    "    if not model_file.exists():\n",
    "        print(f\"Skipping {model_dir.name} - model file not found\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n[{i+1}/{len(model_dirs)}] Evaluating {model_dir.name}...\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        eval_results = evaluate_model(\n",
    "            model_path=str(model_file).replace('.zip', ''),  # PPO.load adds .zip automatically\n",
    "            env=env,\n",
    "            num_episodes=30,\n",
    "            fruitbot_reward_positive=fruitbot_reward_positive,\n",
    "            fruitbot_reward_negative=fruitbot_reward_negative,\n",
    "            fruitbot_reward_wall_hit=fruitbot_reward_wall_hit\n",
    "        )\n",
    "        \n",
    "        # Store summary results\n",
    "        results.append({\n",
    "            'model_name': model_dir.name,\n",
    "            'mean_reward': eval_results['mean_reward'],\n",
    "            'std_reward': eval_results['std_reward'],\n",
    "            'mean_steps': eval_results['mean_steps'],\n",
    "            'mean_good_food': eval_results['mean_good_food'],\n",
    "            'mean_bad_food': eval_results['mean_bad_food'],\n",
    "            'mean_wall_hits': eval_results['mean_wall_hits'],\n",
    "            'episodes_data': eval_results['episodes_data']\n",
    "        })\n",
    "        \n",
    "        print(f\"  Mean reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}\")\n",
    "        print(f\"  Mean steps: {eval_results['mean_steps']:.1f}\")\n",
    "        print(f\"  Mean good food: {eval_results['mean_good_food']:.2f}\")\n",
    "        print(f\"  Mean bad food: {eval_results['mean_bad_food']:.2f}\")\n",
    "        print(f\"  Mean wall hits: {eval_results['mean_wall_hits']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_dir.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Evaluation complete! Evaluated {len(results)} models.\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7feeef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'model_name': r['model_name'],\n",
    "        'mean_reward': r['mean_reward'],\n",
    "        'std_reward': r['std_reward'],\n",
    "        'mean_steps': r['mean_steps'],\n",
    "        'mean_good_food': r['mean_good_food'],\n",
    "        'mean_bad_food': r['mean_bad_food'],\n",
    "        'mean_wall_hits': r['mean_wall_hits']\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "# Sort by mean reward (descending)\n",
    "summary_df = summary_df.sort_values('mean_reward', ascending=False)\n",
    "\n",
    "print(\"Summary of all models (sorted by mean reward):\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv('model_evaluation_summary.csv', index=False)\n",
    "print(\"\\nResults saved to 'model_evaluation_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91290f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: View detailed episodes for the best model\n",
    "if results:\n",
    "    best_model = summary_df.iloc[0]['model_name']\n",
    "    best_model_data = next(r for r in results if r['model_name'] == best_model)\n",
    "    \n",
    "    print(f\"Detailed episodes for best model: {best_model}\\n\")\n",
    "    episodes_df = pd.DataFrame(best_model_data['episodes_data'])\n",
    "    print(episodes_df.head(10))\n",
    "    \n",
    "    # Save detailed data for all models\n",
    "    all_episodes = []\n",
    "    for r in results:\n",
    "        for ep_data in r['episodes_data']:\n",
    "            all_episodes.append({\n",
    "                'model_name': r['model_name'],\n",
    "                **ep_data\n",
    "            })\n",
    "    \n",
    "    all_episodes_df = pd.DataFrame(all_episodes)\n",
    "    all_episodes_df.to_csv('model_evaluation_detailed.csv', index=False)\n",
    "    print(f\"\\nDetailed episode data saved to 'model_evaluation_detailed.csv'\")\n",
    "    print(f\"Total episodes: {len(all_episodes_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen_env_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
