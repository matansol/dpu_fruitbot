{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99c3188",
   "metadata": {},
   "source": [
    "### To build the procgen\n",
    "in terminal:\n",
    "python -c \"from procgen.builder import build; build()\"\n",
    "\n",
    "**If build fails with LNK1168 error:**\n",
    "1. Restart Jupyter kernel\n",
    "2. Close all Python processes\n",
    "3. Delete .build folder and rebuild\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**After installation, restart the Jupyter kernel!**\n",
    "\n",
    "Note: We use sb3 2.0.0a5 which still supports gym 0.23.1 before the gymnasium migration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17dc14c",
   "metadata": {},
   "source": [
    "### Prevent Auto-Build (Use Pre-Built DLLs Only)\n",
    "\n",
    "Set environment variable to skip automatic rebuilding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509e522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCGEN_NO_BUILD set - will use pre-built DLLs only\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Prevent procgen from auto-rebuilding (must be set BEFORE importing procgen)\n",
    "os.environ['PROCGEN_NO_BUILD'] = '1'\n",
    "print(\"PROCGEN_NO_BUILD set - will use pre-built DLLs only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83c4eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\__init__.py\n",
      "Location: .\n",
      "Version: 0.10.7+11f4fd4\n",
      "Procgen module path: c:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import procgen\n",
    "import os\n",
    "\n",
    "# This should point to your local directory\n",
    "print(procgen.__file__)\n",
    "# Expected: C:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\__init__.py\n",
    "\n",
    "# Check the installation location (modern method)\n",
    "import importlib.metadata\n",
    "try:\n",
    "    dist = importlib.metadata.distribution('procgen')\n",
    "    print(f\"Location: {dist.locate_file('')}\")\n",
    "    print(f\"Version: {dist.version}\")\n",
    "except importlib.metadata.PackageNotFoundError:\n",
    "    print(\"procgen package not found in installed packages\")\n",
    "\n",
    "# Alternative: check via module path\n",
    "import procgen\n",
    "print(f\"Procgen module path: {os.path.dirname(procgen.__file__)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368653f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building procgen...done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matan\\anaconda3\\envs\\rl_env\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:437: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  th_object = th.load(file_content, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PPO model from models/fruitbot/20251119-125849_easy/ppo_final.zip\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m---> 45\u001b[0m     obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# small delay so human viewer can follow\u001b[39;00m\n\u001b[0;32m     48\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.02\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a trained PPO model and run it on the existing `env`.\n",
    "# If the model file is missing, fall back to random actions.\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "import gym, time, procgen\n",
    "from procgen.wrappers import make_fruitbot_basic\n",
    "\n",
    "model_path = \"models/fruitbot/20251119-125849_easy/ppo_final.zip\"\n",
    "\n",
    "# Control which level to test on\n",
    "START_LEVEL = 0  # Change this to test different levels (0, 1, 2, ...)\n",
    "NUM_LEVELS = 1   # Set to 1 for single level, 10 for training set, 0 for random\n",
    "\n",
    "env = gym.make('procgen-fruitbot-v0', \n",
    "               render_mode='human', \n",
    "               distribution_mode='easy',\n",
    "               )\n",
    "env = make_fruitbot_basic(env)\n",
    "obs = env.reset()\n",
    "if os.path.exists(model_path):\n",
    "    model = PPO.load(model_path)\n",
    "    print(f\"Loaded PPO model from {model_path}\")\n",
    "else:\n",
    "    model = None\n",
    "    print(f\"Model not found at {model_path}. Falling back to random actions.\")\n",
    "\n",
    "\n",
    "# Run for a number of steps (or until user closes the viewer)\n",
    "max_steps = 200\n",
    "for step in range(max_steps):\n",
    "    if model is not None:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    result = env.step(action)\n",
    "    # handle both gym (obs, rew, done, info) and gymnasium (obs, rew, done, truncated, info)\n",
    "    if len(result) == 5:\n",
    "        obs, rew, done, truncated, info = result\n",
    "    else:\n",
    "        obs, rew, done, info = result\n",
    "        truncated = False\n",
    "\n",
    "    if done or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "    # small delay so human viewer can follow\n",
    "    time.sleep(0.02)\n",
    "\n",
    "print(\"Run finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4875809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matan\\anaconda3\\envs\\rl_env\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:437: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  th_object = th.load(file_content, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PPO model from models/fruitbot/20251119-125849_easy/ppo_final.zip\n",
      "Run finished\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load a trained PPO model and run it on the existing `env`.\n",
    "# If the model file is missing, fall back to random actions.\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "import gym, time, procgen\n",
    "from procgen.wrappers import make_fruitbot_basic\n",
    "\n",
    "model_path = \"models/fruitbot/20251123-133459_easy/ppo_final.zip\"\n",
    "\n",
    "# Control which level to test on\n",
    "START_LEVEL = 0  # Change this to test different levels (0, 1, 2, ...)\n",
    "NUM_LEVELS = 1   # Set to 1 for single level, 10 for training set, 0 for random\n",
    "\n",
    "env = gym.make('procgen-fruitbot-v0', \n",
    "               render_mode='human', \n",
    "               distribution_mode='easy',\n",
    "               )\n",
    "env = make_fruitbot_basic(env)\n",
    "obs = env.reset()\n",
    "if os.path.exists(model_path):\n",
    "    model = PPO.load(model_path)\n",
    "    print(f\"Loaded PPO model from {model_path}\")\n",
    "else:\n",
    "    model = None\n",
    "    print(f\"Model not found at {model_path}. Falling back to random actions.\")\n",
    "\n",
    "\n",
    "# Run for a number of steps (or until user closes the viewer)\n",
    "max_steps = 200\n",
    "for step in range(max_steps):\n",
    "    if model is not None:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    result = env.step(action)\n",
    "    # handle both gym (obs, rew, done, info) and gymnasium (obs, rew, done, truncated, info)\n",
    "    if len(result) == 5:\n",
    "        obs, rew, done, truncated, info = result\n",
    "    else:\n",
    "        obs, rew, done, info = result\n",
    "        truncated = False\n",
    "\n",
    "    if done or truncated:\n",
    "        obs = env.reset()\n",
    "\n",
    "    # small delay so human viewer can follow\n",
    "    time.sleep(0.02)\n",
    "\n",
    "print(\"Run finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea699b8",
   "metadata": {},
   "source": [
    "## CNN architecuter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c63c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FULL ACTOR-CRITIC NETWORK\n",
      "============================================================\n",
      "ActorCriticCnnPolicy(\n",
      "  (features_extractor): NatureCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (pi_features_extractor): NatureCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (vf_features_extractor): NatureCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential()\n",
      "    (value_net): Sequential()\n",
      "  )\n",
      "  (action_net): Linear(in_features=512, out_features=4, bias=True)\n",
      "  (value_net): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "PARAMETER COUNT\n",
      "============================================================\n",
      "Feature extractor: 600,736 params\n",
      "Actor head: 2,052 params\n",
      "Critic head: 513 params\n",
      "Total: 603,301 params\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "\n",
    "# Load your trained model\n",
    "model = PPO.load(\"models/fruitbot/20251117-143015/ppo_final.zip\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FULL ACTOR-CRITIC NETWORK\")\n",
    "print(\"=\"*60)\n",
    "print(model.policy)\n",
    "\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAMETER COUNT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Feature extractor: {count_parameters(model.policy.features_extractor):,} params\")\n",
    "print(f\"Actor head: {count_parameters(model.policy.action_net):,} params\")\n",
    "print(f\"Critic head: {count_parameters(model.policy.value_net):,} params\")\n",
    "print(f\"Total: {count_parameters(model.policy):,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36ab90",
   "metadata": {},
   "source": [
    "### Capture Viewer-Quality Images and Videos\n",
    "\n",
    "Capture the exact same RGB frames that the Gym3 viewer shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "898ed1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matan\\anaconda3\\envs\\procgen_env_clone\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building procgen...done\n",
      "done\n",
      "Wrapper 0: StayBonusWrapper\n",
      "Wrapper 1: DiscreteActionWrapper\n",
      "Wrapper 2: OrderEnforcing\n",
      "Wrapper 3: RenderableToGymEnv\n",
      "Wrapper 4: HighResRenderWrapper\n",
      "Wrapper 5: ExtractDictObWrapper\n",
      "Base env: ProcgenGym3Env\n",
      "Wrapper 0: StayBonusWrapper\n",
      "Wrapper 1: DiscreteActionWrapper\n",
      "Wrapper 2: OrderEnforcing\n",
      "Wrapper 3: RenderableToGymEnv\n",
      "Wrapper 4: HighResRenderWrapper\n",
      "Wrapper 5: ExtractDictObWrapper\n",
      "Base env: ProcgenGym3Env\n"
     ]
    }
   ],
   "source": [
    "# Capture high-resolution frames using render()\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "import gym, procgen\n",
    "from procgen.wrappers import make_fruitbot_basic\n",
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Create outputs directory\n",
    "os.makedirs(\"screenshots\", exist_ok=True)\n",
    "\n",
    "# Create environment with rgb_array rendering to enable render() method\n",
    "env = gym.make('procgen-fruitbot-v0', \n",
    "               render_mode='rgb_array',\n",
    "               distribution_mode='easy')\n",
    "env = make_fruitbot_basic(env)\n",
    "\n",
    "# After creating the environment, you can inspect the wrapper chain like this:\n",
    "def print_wrapper_chain(env):\n",
    "    i = 0\n",
    "    while hasattr(env, 'env'):\n",
    "        print(f\"Wrapper {i}: {type(env).__name__}\")\n",
    "        env = env.env\n",
    "        i += 1\n",
    "    print(f\"Base env: {type(env).__name__}\")\n",
    "\n",
    "print_wrapper_chain(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72414ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PPO model from models/fruitbot/20251123-133459_easy/ppo_final.zip\n",
      "Observation shape: (64, 64, 3)\n",
      "Observation dtype: uint8\n",
      "upscale image= [0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Capture initial high-res frame\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial rendered frame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\wrappers.py:224\u001b[0m, in \u001b[0;36mStayBonusWrapper.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Delegate render to the base environment\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\wrappers.py:162\u001b[0m, in \u001b[0;36mDiscreteActionWrapper.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Delegate render to the base environment\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matan\\anaconda3\\envs\\procgen_env_clone\\lib\\site-packages\\gym\\core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\gym_adapter.py:67\u001b[0m, in \u001b[0;36mRenderableToGymEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     63\u001b[0m render_wrapper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_high_res_render_wrapper \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_wrapper\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render_wrapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Use the high-resolution render wrapper\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mrender_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Fallback: try to get observation directly\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\wrappers.py:310\u001b[0m, in \u001b[0;36mHighResRenderWrapper.get_image\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    307\u001b[0m         rgb_obs \u001b[38;5;241m=\u001b[39m rgb_obs\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# Upscale the image\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upscale_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matan\\master_thesis\\rl_envs\\procgen\\procgen\\wrappers.py:328\u001b[0m, in \u001b[0;36mHighResRenderWrapper._upscale_image\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    326\u001b[0m pil_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupscale image=\u001b[39m\u001b[38;5;124m\"\u001b[39m, img)\n\u001b[1;32m--> 328\u001b[0m new_size \u001b[38;5;241m=\u001b[39m (\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_factor, \n\u001b[0;32m    329\u001b[0m            img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_factor)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    332\u001b[0m     resample \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mNEAREST\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = \"models/fruitbot/20251123-133459_easy/ppo_final.zip\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = PPO.load(model_path)\n",
    "    print(f\"Loaded PPO model from {model_path}\")\n",
    "else:\n",
    "    model = None\n",
    "    print(\"Using random actions\")\n",
    "\n",
    "# Reset environment\n",
    "obs = env.reset()\n",
    "if isinstance(obs, tuple):\n",
    "    obs = obs[0]\n",
    "\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "print(f\"Observation dtype: {obs.dtype}\")\n",
    "\n",
    "# Now render() will return high-resolution upscaled frames\n",
    "frames = []\n",
    "\n",
    "# Capture initial high-res frame\n",
    "frame = env.render()\n",
    "print(f\"Initial rendered frame: {frame}\")\n",
    "if frame is not None:\n",
    "    print(f\"Rendered frame shape: {frame.shape}\")\n",
    "    frames.append(frame)\n",
    "\n",
    "# Run a few steps and capture high-res frames\n",
    "for step in range(5):\n",
    "    # Take action\n",
    "    if model is not None:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    \n",
    "    result = env.step(action)\n",
    "    if len(result) == 5:\n",
    "        obs, rew, done, truncated, info = result\n",
    "        done = done or truncated\n",
    "    else:\n",
    "        obs, rew, done, info = result\n",
    "    \n",
    "    # Get high-resolution rendered frame\n",
    "    frame = env.render()\n",
    "    if frame is not None:\n",
    "        frames.append(frame)\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        if isinstance(obs, tuple):\n",
    "            obs = obs[0]\n",
    "\n",
    "# Save first frame as image\n",
    "if frames:\n",
    "    Image.fromarray(frames[0]).save(\"screenshots/fruitbot_highres_frame.png\")\n",
    "    print(f\"Saved high-res screenshot to screenshots/fruitbot_highres_frame.png\")\n",
    "    \n",
    "    # Save all frames as video\n",
    "    video_path = \"screenshots/fruitbot_highres_gameplay.mp4\"\n",
    "    imageio.mimsave(video_path, frames, fps=15)\n",
    "    print(f\"Saved high-res video to {video_path}\")\n",
    "else:\n",
    "    print(\"No frames captured!\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc19a4",
   "metadata": {},
   "source": [
    "### Record Full Episodes with Viewer-Quality Video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f3004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record full episodes with viewer-quality video\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "import gym, procgen\n",
    "from procgen.wrappers import make_fruitbot_basic\n",
    "import imageio\n",
    "\n",
    "model_path = \"models/fruitbot/20251123-133459_easy/ppo_final.zip\"\n",
    "\n",
    "# Create videos directory\n",
    "os.makedirs(\"videos\", exist_ok=True)\n",
    "\n",
    "# Create environment with rgb_array rendering\n",
    "env = gym.make('procgen-fruitbot-v0', \n",
    "               render_mode='rgb_array',\n",
    "               distribution_mode='easy')\n",
    "env = make_fruitbot_basic(env)\n",
    "\n",
    "# Load model\n",
    "if os.path.exists(model_path):\n",
    "    model = PPO.load(model_path)\n",
    "    print(f\"Loaded PPO model from {model_path}\")\n",
    "else:\n",
    "    model = None\n",
    "    print(\"Using random actions\")\n",
    "\n",
    "# Record episodes\n",
    "num_episodes = 3\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    frames = []\n",
    "    \n",
    "    # Capture initial high-res frame\n",
    "    frame = env.render()\n",
    "    if frame is not None:\n",
    "        frames.append(frame)\n",
    "    \n",
    "    while not done:\n",
    "        # Get action\n",
    "        if model is not None:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Step environment\n",
    "        result = env.step(action)\n",
    "        if len(result) == 5:\n",
    "            obs, rew, done, truncated, info = result\n",
    "            done = done or truncated\n",
    "        else:\n",
    "            obs, rew, done, info = result\n",
    "        \n",
    "        episode_reward += rew\n",
    "        \n",
    "        # Capture high-res frame\n",
    "        frame = env.render()\n",
    "        if frame is not None:\n",
    "            frames.append(frame)\n",
    "    \n",
    "    # Save video\n",
    "    if frames:\n",
    "        video_path = f\"videos/fruitbot_highres_episode_{episode}.mp4\"\n",
    "        imageio.mimsave(video_path, frames, fps=15)\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} - Reward: {episode_reward} - Frames: {len(frames)} - Saved to {video_path}\")\n",
    "    else:\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} - No frames captured!\")\n",
    "\n",
    "env.close()\n",
    "print(\"\\nAll high-resolution videos saved to 'videos' folder\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen_env_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
